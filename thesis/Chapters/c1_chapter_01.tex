%!TEX root = ../thesis.tex

\chapter{Теоретичні відомості}
\label{chap:review}
В цьому розділі викладено основні теоретичні результати і визначення формальної теорії мов, деякі визначення з теорії прихованих моделей Маркова та теорії обробки природних мов.

\section{Природні мови}

З часу виникнення перших природних мов пройшли десятки тисяч років, проте сьогодні важко уявити засіб комунікації більш дієвий ніж мова.
Вивченням природних мов займається лінгвістика, що об'єднує безліч напрямків вивчення аспектів природних мов. Серед них особливо можна
виділити граматику - науку про структуру мови. В свою чергу граматика поділяється на синтаксис і морфологію. Морфологія вивчає структуру
окремих слів, що складаються з літер алфавіту, тоді як синтаксис вивчає правила, за якими слова складають граматично правильні речення.
Вивченням змістовних та інформативних характеристик мови займається семантика. З різними поглядами на співвідношення граматики
та семантики пов'язано чимало дискусій. В 50х роках американський лінгвіст та публіцист Ноам Хомський публікує праці з лінгвістики,
що свого роду перевертають звичні уявлення про мову, вводячи поняття генеративної лінгвістики та генеративної граматики. За уявленнями
Хомського всі мовленнєві структури мають над собою більш абстрактні і загальні структури, а правила за якими більш абстрактні структури
породжують менш абстрактні і утворюють генеративну граматику. Тобто центральним місцем в лінгвістиці згідно Хомським є граматика, тоді
як семантика мови в дечому визначається граматикою та її глибинною структурою[4,5]. Цей підхід сьогодні в середовищі багатьох європейських лінгвістів визнаний не зовсім сучасним і некоректним, оскільки існує дуже багато контрприкладів в природних мовах, які не пояснюються або пояснюються слабо з точки зору теорій Хомського[22].

З розвитком сучасного штучного інтелекту тісно пов'язана проблема аналізу та синтезу природних мов. Яким чином машина, яка не має власної
свідомості, а тільки запрограмований набір логічних команд може ефективно розрізняти правильні граматично і семантично фрази від
не правильних? Яким чином вона може утворювати осмислені фрази та прогнозувати слова які може сказати співбесідник? Алан Тьюрінг в своїх
роботах ввів тест, який повинен пройти справжній штучний інтелект. Цей тест включає позитивні відповіді на згадані вище питання,
тому обробка природних мов є надзвичайно важливою гілкою computer science[8].

Теорія граматик, якій значною мірою є присвячена дана наукова робота, важлива не тільки при обробці природних мов. В біології теорія граматик
застосовується при аналізі структур ланцюгів нуклеотидів в транспортній РНК[11]. В теорії компіляторів мов програмування при побудові синтаксичних парсерів.
В криптоаналізі при побудові моделей відкритого тексту. В теорії розпізнавання образів при виділенні глибоких структур в зображеннях. І це
далеко не повний перелік застосування теорії граматик.

\section{Граматики, формальні мови та ієрархія Хомського}

Сучасна формалізація поняття мови почалось з робіт Ноама Хомського в 50-60х роках. Його метою була повна формалізація англійської мови, перенесення
її граматики і правил на мову логіки та математики. Повної формалізації не досягнуто і до сьогодні, проте було запропоновано багато концепцій,
які дуже важливі сьогодні. Наведемо формальні базові визначення з теорії формальних мов та граматик[1].

Нехай задано деякий алфавіт, тобто скінчену множину $ \Sigma $.
\begin{definition}
  \textit{Мовою $L$ над алфавітом $\Sigma$} називається деяка підмножина $ L \subset \Sigma^* $. Де $ \Sigma^* $ - замикання Кліні множини $ \Sigma $,
  тобто всі можливі скінчені конкатенації елементів алфавіту $ \Sigma $. Такі конкатенації, тобто ланцюжки послідовно записаних елементів алфавіту називаються
  \textit{словами} або \textit{реченнями}, в залежності від контексту.
\end{definition}
Зауважимо що множина $ \Sigma^* $ нескінченна, окрім випадку коли алфавіт порожній. Наведемо приклад мови:
\begin{example}
Нехай наш алфавіт:
$$ \Sigma = \{ a,\ e,\ h,\ t \} $$
Тоді деяка мова $ L $ над цим алфавітом:
$$ L = \{ ae,\ the,\ ht,\ ttt,\ aaaa \} $$
Зауважимо, що в даному випадку, коли алфавіт складається з літер, елементи нашої мови $L$ називають \textit{словами}. Якщо елементи алфавіту - власне
самі слова, то елементи мови прийнято називати \textit{реченнями}(до прикладу в теорії синтаксису). Наприклад:
$$ \Sigma = \{ Bill,\ Mary,\ loves, `\ ` \} $$
$$ L = \{ Bill\ loves\ Mary,\ Mary\ loves\ Bill \} $$
\end{example}

З поняттям мови тісно пов'язане поняття \textit{граматики}. Неформально граматика - множина правил, що породжують мову. Для формального визначення
граматики потрібні визначення термінальних та нетермінальних символів.

\begin{definition}
  \textit{Термінальним символом або терміналом} називають елемент $ x \in \Sigma $, де $\Sigma$
  - звичний нам алфавіт, з якого будуть складатись слова мови, які породжує граматика. Алфавіт $\Sigma$ називають
  \textit{алфавітом термінальних символів}. Часто в множину терміналів включають порожній символ $ \epsilon $.
\end{definition}

\begin{definition}
  \textit{Нетермінальним символом, нетерміналом або змінною} називають елемент $ x \in N $, де N - алфавіт, що предствляє деякі категорії або абстракції мови, що не
  входять безпосередньо в алфавіт, з якого будуються слова мови. $N$ називають \textit{алфавітом нетермінальних символів}.
\end{definition}

\begin{definition}
  \textit{Граматикою} $ G $ називають впорядковану четвірку $ G = \big\langle N, \Sigma, S, R \big\rangle $ Де $N$ - алфавіт нетермінальних символів, $\Sigma$ - алфавіт термінальних символів,
  $S$ - стартовий нетермінальний символ, $R$ - множина \textit{продукцій} граматики, де продукціями називають правила заміни ланцюжків символів виду:
  $$ A \rightarrow B $$
  Де $ A,B \in \left(N \cup \Sigma \right)^* $
\end{definition}
\begin{definition}
  Слово $ w \in \left(N \cup \Sigma \right)^* $ називається \textit{породженим} з слова $ v \in \left(N \cup \Sigma \right)^* $ в граматиці $G$, якщо існує правило виводу $ g \in G $, застосування якого до слова $v$ продукує слово $w$. Позначається $v \Rightarrow w$.
  Схожим чином будем позначати $v \Rightarrow^* w$, якщо існує ланцюг правил виводу, що дозволяють з слова $v$ отримати слово $w$, послідовно застосовуючи їх.
\end{definition}
Граматики перетворюють стартовий нетермінал $S$ згідно правил $G$, послідовно замінюючи ліві частини продукцій на праві. Наведемо приклад:
\begin{example}
  Нехай дана граматика
  $$ G = \big\langle \{ S \}, \{ a,\ b, \epsilon \}, S, \{ S \rightarrow aSa,\ S \rightarrow bSb,\ S \rightarrow a,\ S \rightarrow b,\ S \rightarrow \epsilon \} \big\rangle $$
  Мова $L(G)$, породжувана цією граматикою складається з ланцюжків букв $a,\ b$, які однаково читаються зправа на ліво і зліва направо,
  це так звані \textit{паліндроми}.
\end{example}
З поняттям граматики тісно пов'язане поняття мови, що вона породжує.
\begin{definition}
  Кажуть, що граматика $ G = \big\langle N, \Sigma, S, R \big\rangle $ породжує мову $ L(G) $ тоді і тільки тоді коли $ \forall x \in L(G)\ \exists \ S \Rightarrow^* x $
\end{definition}
Н. Хомський класифікував всі мови за деякими ознаками строгості граматик які їх породжують:
\begin{center}
  \textit{
  Вільні мови\\
  Контекстно-залежні мови\\
  Контекстно-вільні мови\\
  Регулярні мови
  }
\end{center}

\textit{Регулярні мови} - найбільш вузький клас мов в ієрархії. Вони розпізнаються детермінованими скінченими автоматами, тому вони стали дуже поширеними в мовах програмування для написання регулярних виразів - шаблонів пошуку і допускання рядків.

\textit{Контекстно-вільні мови} - один із найцікавіших класів мов, що достатньо добре формалізують природні мови, ланцюжки нуклеотидів в ДНК, мови програмування та багато іншого. Дана наукова робота присвячена вивченню аспектів контекстно-вільних мов та граматик, тому далі зупинимось детальніше на них.
\begin{definition}
\textit{Контекстно-вільною граматикою} $G$ називається граматика $ G = \big\langle N, \Sigma, S, R \big\rangle $, всі правила виводу $R$ якої мають вигляд:
$$ A \rightarrow \gamma,\ A \in N,\ \gamma \in \left(N \cup \Sigma \right)^* $$
Відповідно \textit{контекстно-вільною мовою} називається мова, що породжена деякою контекстно-вільною граматикою.
\end{definition}
Зауважимо, що назва контекстно-вільної граматики дещо оманлива і аж ніяк не вказує на те, що слова не залежать від контексту,
в яких вони стоять в реченні, а лише на той факт, що граматичний вивід не залежить від контексту, який оточує нетермінал.
Н. Хомський запропонував КВ-граматики для опису граматики англійської. До кінця це не вдалось нікому, проте моделі англійської, як КВ-мови виявились досить непоганими. Виявилось, що не тільки англійська може бути добре описана КВ-граматикою, але і багато інших природних мов, проте далеко не всі, оскільки відомо декілька природних мов, що точно не є контекстно-вільними. В лінгвістиці речення зазвичай представляють у вигляді двох груп слів - групи підмета(\textit{noun phrase}) та групи присудка(\textit{verb phrase}). Формальний граматичний еквівалент: $S \rightarrow NP\ VP$. Варто зазначити, що клас контекстно-вільних мов досить широкий і еквівалентний недетермінованим автоматам з магазинною пам'яттю(стеком)[1]. Це дозволяє на практиці ефективно створювати синтаксичні парсери контекстно-вільних мов.

\textit{Контекстно-залежні мови} - мови дещо більш складні ніж контекстно-вільні, оскільки граматики що їх породжують, враховують термінальні та нетермінальні символи, що оточують нетермінал в правилах виводу, таким чином контекст впливає на граматичні породження. Одним із найцікавіших прикладів серед природних мов є швейцарський діалект німецької, де речення
\textit{... das mer em Hans es huus hälfed aastriiche}, що перекладається \textit{... що ми допомогли Гансу розмалювати будинок} є прикладом перехресної залежності між іменниками та дієсловами. Дослівний переклад на українську - \textit{... що ми Гансу будинок допомогли розмалювати}. Саме ці синтаксично-розірвані групи підмета та присудка і створюють контекстну залежність в граматиці.

\textit{Вільні мови} - мови, що породжені довільною граматикою. В літературі по формальній теорії алгоритмів цей клас також називають \textit{рекурсивно-перечислюваними мовами}. Еквівалент - машина Т'юрінга.

\section{Нормальні форми контекстно-вільних граматик}
Для зручності представлення, парсингу мов та доведення теорем контекстно-вільні граматики часто представляють у вигляді, де правила виводу мають певний специфічний вигляд.
\begin{definition}
Граматика знаходиться у \textit{нормальній формі Хомського (CNF)}, якщо всі правила виводу мають вигляд:
$$ A \rightarrow a $$
$$ A \rightarrow A_1A_2 $$
Де $ a \in \Sigma,\ A, A_1, A_2 \in N $
\end{definition}
\begin{theorem}
Кожна граматика у нормальній формі Хомського є контекстно-вільною і кожну контекстно-вільну граматику можна привести до нормальної форми Хомського[1].
\end{theorem}

\begin{definition}
Граматика знаходиться у \textit{нормальній формі Грейбах (GNF)}, якщо всі правила виводу мають вигляд:
$$ A \rightarrow a A_1 A_2 \dots A_n $$
$$ S \rightarrow \epsilon $$
Де $ a \in \Sigma,\ A \in N,\ A_1, A_2 \in N \setminus \{S\}, n \in \{ 0,1,\dots \} $
\end{definition}

\begin{theorem}
Кожна граматика у нормальній формі Грейбах є контекстно-вільною і кожну контекстно-вільну граматику можна привести до нормальної форми Грейбах[1, 7].
\end{theorem}

В нашій роботі будемо використовувати один із варіантів нормальної форми Грейбах, а саме \textit{2-нормальну форму Грейбах}.
\begin{definition}
Граматика знаходиться у \textit{2-нормальній формі Грейбах (2-GNF)}, якщо всі правила виводу мають вигляд:
$$ A \rightarrow a A_1 A_2 $$
$$ A \rightarrow a A_1 $$
$$ A \rightarrow a $$
$$ S \rightarrow \epsilon $$
Де $ a \in \Sigma,\ A \in N,\ A_1, A_2 \in N \setminus \{S\} $
\end{definition}

\begin{theorem}
Кожна граматика у 2-нормальній формі Грейбах є контекстно-вільною і кожну контекстно-вільну граматику можна привести до 2-нормальної форми Грейбах[16, 17].
\end{theorem}

Наступна властивість нормальної форми Грейбах буде в подальшому дуже корисною для побудови моделей
\begin{claim}
  Для породження речення довжини $L$ в граматиці, що знаходиться в нормальній формі Грейбах потрібно рівно $L$ застосувань правил виводу.
\end{claim}
\begin{proof}
  Всі правила виводу граматики в нормальній формі Грейбах мають вигляд: $ A \rightarrow a \gamma $, де $\gamma$ - довільна послідовність нетерміналів окрім стартового символу. Таким чином стан граматичного виводу $w A$($w$ - ланцюжок терміналів, що були виведені граматикою раніше) після застосування правила стане $w a \gamma$, кількість терміналів в стані збільшиться на одиницю. Таким чином, застосовуючи правила виводу $n$ разів в лівій частині стану граматичного виводу буде знаходитись $n$ терміналів.
\end{proof}

Ця властивість дозволила свого часу суттєво розвити теорію синтаксичних парсерів та компіляторів мов програмування, оскільки для парсингу речень, що належать мові яка породжена граматикою в нормальній формі Грейбах top-down парсеру необхідна кількість кроків, що співпадає з довжиною речення.

Вибір саме 2-нормальної форми Грейбах для подальшого дослідження обумовленений обмеженням кількості нетерміналів в правих частинах правил виводу, що робити цю нормальну форму зручною для подальшого дослідження.
Також існують поліноміальні алгоритми зведення КВ-граматики до нормальних форм Хомського та Грейбах, але в даному розділі приводити їх не будемо[1, 16, 17].
\section{Елементи тензорного числення}
Далі нам у роботі будуть потрібні дещо специфічні об'єкти, що в нашому застосуванні будуть нагадувати багатовимірні матриці.
\begin{definition}
\textit{Тензором} рангу $(n,m)$ над векторним простором $V$ будемо називати елемент тензороного добутку:
$$ V^{\otimes n} \otimes V^{*\otimes m} $$
Де $V^*$ - спряжений простір до $V$(простір всіх лінійних функціоналів над $V$). $V^{\otimes n}$ - тензорний добуток $V$ на самого себе n раз.
\end{definition}
До прикладку тензори рангу (1,0) - звичайні вектори із $V$, тензори рангу (1,1) - лінійні перетворення простору $V$(матриці). Тензори рангу (2,1) та (1,2) подібні до трьох-вимірних матриць. Вони часто будуть застосовуватись в нашій роботі.

Часто рангом тензора називають суму $n+m$. Далі будемо користуватися саме такою нотацією для рангу.
\section{Класичні моделі природних мов}
Відомо що природні мови дуже не рівноімовірні, ентропія(кількість інформації на символ тексту) природних мов набагато менша за ентропію рівноімовірного ансамблю, тому існує багато статистичних властивостей мов, які досить добре прослідковуються в межах тематичних текстів.
Клод Шенон в своїх роботах ввів декілька моделей природних мов, що знайшли широке застосування в криптографії, оскільки вони досить добре описують базові статистичні властивості мови, що важливо при криптоаналізі. Розглянемо моделі, що були ним запропоновані:
\begin{itemize}
  \item Модель \textit{M0} - текст представляє послідовність незалежних випадкових символів, що мають рівноімовірний розподіл на алфавіті: $P(x_k = i) = 1/n$, $n$ - розмір алфавіту. Такий текст буде мати максимальну ентропію на символ тексту.
  \item Модель \textit{M1} - текст представляє собою послідовність випадкових символів алфавіту, імовірності яких збігаються з оригінальними їх імовірностями появи у мові. Тобто $ P(x_i = a) = p_a $, де $x_i$ - і-тий символ тексту $p_a$ - оцінка імовірності появи символу $a$ в тексті, отримана статистичним шляхом.
  \item Модель \textit{M2} - текст представляє собою послідовність випадкових біграм, тобто пар символів алфавіту, розподіл яких збігається з оригінальним розподілом біграм у мові $ P(x_i = ab) = p_{ab} $, де $x_i$ - і-та пара символів у тексті(пари не перетинаються), $p_{ab}$ - оцінка імовірності біграми $ab$, отримана статистичним шляхом.
  \item Модель \textit{M3} - текст представляється у вигляді ланцюга Маркова, тобто
  $$ P(x_k | x_{1:k-1}) = P(x_k | x_{k-1}),\ P(x_k = a | x_{k-1} = b) = p_{ab} $$
\end{itemize}
Дані моделі описують мову з дуже простого статистичного боку, враховуючи звичайні неоднорідності вживання різних букв в мові і як правило вони не підходять для більш точного
опису властивостей мови. Такі ж моделі можна побудувати, заміняючи букви на слова, проте оскільки кількість слів у мові набагато більша ніж букв, очікувати осмислених текстів не варто.

Одним із способів поєднати формальну теорію мов і граматик та природні статистичні властивості мов є введення стохастичних контекстно-вільних граматик, що враховують як граматичні властивості мови так і деякі семантичні, що продиктовані природніми статистичними особливостями.
\section{Стохастичні контекстно-вільні граматики}
 Дамо формальне визначення стохастичних контекстно-вільних граматик.
\begin{definition}
  \textit{Стохастичною(імовірнісною) контекстно-вільною граматикою} $ G $ називають впорядковану п'ятірку
  $ G = \big\langle N, \Sigma, S, R, \mathbb{P}\big\rangle $ Де $ G' = \big\langle N, \Sigma, S, R\big\rangle $ - контекстно-вільна граматика,
  $N = \{A_1,\dots,A_m \}$, $S = A_1$, а $\mathbb{P} = \{ \mathbb{P}_1,\dots,\mathbb{P}_m \}$ - сімейство імовірнісних розподілів на $G$,
  таке що на кожній множині продукцій, що починаються з одинакового нетерміналу:
  $$ G_i = \{ g \in G | g\colon A_i \rightarrow \gamma_i,\ \gamma_i \in \left(N \cup \Sigma \right) \} $$
  задано імовірнісний розподіл $\mathbb{P}_i,\ i = \overline{1,\ m}$
\end{definition}
Також введемо поняття \textit{стохастичної мови}, що взагалі кажучи може бути породжена будь якою стохастичною граматикою, а не тільки контекстно-вільними.
\begin{definition}
  Кажуть, що стохастична граматика $ G = \big\langle N, \Sigma, S, R, \mathbb{P}\big\rangle $ породжує стохастичну мову $ L(G) $ тоді і тільки тоді коли $ \forall x \in L(G) \exists S \Rightarrow^* x $ та на $ L(G) $ задано імовірнісний розподіл, індукований граматикою $ G $: $ \forall x \in L(G) \colon P(x) = P(S \Rightarrow^* x) $
\end{definition}

Стохастичні контекстно-вільні граматики є досить потужним засобом для моделювання досить різних за природою структур, не враховуючи природні мови це можуть бути ланцюжки ДНК, певні структури в зображеннях, природні фрактали, музичні твори та інше. Варто зазначити що багато результатів в теорії стохастичних граматик були отримані в результаті дослідження саме первинних і вторинних структур транспотрної РНК, в тому числі ідея застосування прихованих моделей Маркова, яку в подальшому будемо розвивати в роботі походить з біоінформатики[11,14].

Для нас особливий інтерес будуть складати контекстно-вільні граматики в 2-нормальній формі Грейбах.

\section{Приховані моделі Маркова}
Для подальшого дослідження також знадобиться апарат прихованих моделей Маркова, тому опишемо їх та окреслимо основні проблеми та способи їх вирішення.
Отже, як відомо ми називаємо стохастичний процес $ \{ X_t,\ t \in \mathbb{N} \}$ \textit{ланцюгом Маркова} з множиною станів $E$, якщо виконується $ \forall t > 1 \colon P(X_{t} = k | X_{1:t-1}) = P(X_{t} = k | X_{t-1}),\ k \in E $. Позначення $ X_{1:t-1} $ трактується як вектор $ \left( X_1,\dots,X_{t-1} \right) $.
Одна із найпоширеніших варіацій - однорідний ланцюг Маркова однозначно задається матрицею переходів між станами $\mathbb{P}$, яка не змінюється для будь яких $n \in \mathbb{N}$ та стартовим розподілом $\pi_0$. Із вигляду матриці $ \mathbb{P} $ можна вивести багато властивостей ланцюга, таких як періодичність, рекурентність та ергодичність. Ланцюги Маркова є ймовірнісними аналогами систем, що задані деякими задачами Коші, тобто диференційними рівняннями разом з початковими умовами, тому вони і стали такими широко розповсюдженими в сучасній математиці, фізиці та економіці.
Одним із найвдаліших застосувань концепції ланцюгів Маркова є приховані моделі Маркова, що виникли з потреб розпізнавання мови на фонетичному рівні та інших проблем, що пов'язані з рядом спостережень, що безпосередньо були наслідком деякого іншого прихованого процесу, який не можна спостерігати. Дамо спочатку визначення прихованої моделі Маркова і наведемо приклади.
\begin{definition}
  \textit{Прихованою моделлю Маркова} з простором латентних(прихованих) станів $E = \{ 1,\dots,n \}$ та простором спостережень $O = \{ 1,\dots,m \}$ називається двохвимірний стохастичний процес $ \{ (X_t, Y_t),\ t \in \mathbb{N} \},\ \ X_t \in E,\ Y_t \in O $, що має властивості:
  \begin{itemize}
    \item $ \{ X_t,\ t \in \mathbb{N} \} $ - однорідний ланцюг Маркова.
    \item $ \forall t \in \mathbb{N} \colon P(Y_t = k | X_{1:t}) = P(Y_t = k | X_t),\ k \in O $
  \end{itemize}
\end{definition}
Тобто неформально, прихована марківська модель - це деякий процес, що залежить від станів ланцюга Маркова, які безпосередньо не можна спостерігати, але можна спостерігати прояви цього ланцюга. Прихована модель Маркова описується двома матрицями - матрицею переходів між латентними станами:
$$ \mathbb{P} = \{ p_{ij} \}_{i,j=1..n},\ p_{ij} = P(X_t = j | X_{t-1} = i) $$
та матрицею спостережень:
$$ \mathbb{T} = \{ q_{ij} \}_{i=1..n, j=1..m},\ p_{ij} = P(Y_t = j | X_t = i) $$

\begin{example}
  Нехає є деякий фонетичний розпізнавач, який на вхід приймає деякий сигнал, що представляє собою послідовність звуків які вимовляє людина. Тоді відносно розпізнавача сигнал може бути уявлений як вихід деякої прихованої моделі Маркова, що в якості прихованого ланцюга має природну послідовність букв в мові. Таким чином, щоб ідентифікувати послідовність букв $X_{1:t}$ за звуками $y_{1:t}$, що ми спостерігаємо достатньо максимізувати імовірність: $ P(X_{1:t} | Y_{1:t} = y_{1:t}) $, дана задача вирішується за допомогою відомого алгоритму Вітербі(\textit{Viterbi algorithm})[13].
\end{example}
Крім вище описаної задачі в рамках теорії прихованих моделей Маркова вирішується задача знаходження найбільш імовірного ланцюжка спостережень $y_{1:t} \colon P(Y_{1:t} = y_{1:t}) \rightarrow max $ за допомогою алгоритму прямого ходу(\textit{Forward algorithm}) та задача ідентифікації параметрів(матриці $\mathbb{P},\ \mathbb{T}$) прихованої марківської моделі(задача навчання) за допомогою алгоритму Баума-Велша(\textit{Baum-Welch algorithm})[9,10] з складністю $\mathcal{O}(nm^2)$, що базується на алгоритмі прямого-зворотнього ходу(\textit{Forward-Backward algorithm})

Далі нам знадобиться деяке узагальнення прихованої моделі Маркова, а саме прихована модель Маркова порядку r за спостереженнями.
\begin{definition}
  \textit{Прихованою моделлю Маркова порядку r} з простором латентних(прихованих) станів $E = \{ 1,\dots,n \}$ та простором спостережень $O = \{ 1,\dots,m \}$ називається двовимірний стохастичний процес $ \{ (X_t, Y_t),\ t \in \mathbb{N} \},\ \ X_t \in E,\ Y_t \in O $, що має властивості:
  \begin{itemize}
    \item $ \{ X_t,\ t \in \mathbb{N} \} $ - однорідний ланцюг Маркова.
    \item $ \forall t \in \mathbb{N} \colon P(Y_t = k | X_{1:t}) = P(Y_t = k | X_{t-r-1:t}),\ k \in O $
  \end{itemize}
\end{definition}

\section{Огляд сучасних підходів до навчання стохастичних контекстно-вільних граматик}
Оглянемо деякі сучасні підходи для навчання стохастичних контекстно-вільних граматик. Найпоширенішим підходом є навчання стохастичної контекстно-вільної граматики в нормальній формі Хомського за допомогою алгоритму \textit{Inside-Outside}[2,3]. Алгоритм в чомусь подібний до алгоритму \textit{Forward-Backward}, проте він має більшу складність $ \mathcal{O}(n^3m^3) $, де $ n,\ m $ - відповідно потужності алфавітів нетермінальних і термінальних символів. Оскільки при великих значеннях $n$ і $m$ складність алгоритму може бути досить значною існує проблема розробки ефективніших методів.

%більше!!!
\chapconclude{\ref{chap:review}}
В розділі було розглянуто проблематику формалізації природних мов, теоретичні результати формальної теорії мов, граматик, теорії прихованих моделей Маркова та їхнє практичне застосування. Зокрема було розглянуто класичні підходи до навчання стохастичних контекстно-вільних граматик та класичні підходи до побудови моделей відкритого тексту.
