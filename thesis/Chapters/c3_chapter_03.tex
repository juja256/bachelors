%!TEX root = ../thesis.tex
\chapter{Практичне застосування запропонованої моделі}

\label{chap:practice}
\section{Сучасні моделі природних мов}
В першому розділі нами було розглянуто класичні моделі природних мов: M0, M1, M2, M3. Розглянемо деякі сучасні підходи до моделювання природних мов.

Більш узагальненим і складним варіантом описаних моделей M0, M1, M2, M3 є n-грамна модель мови(текст утворює ланцюг Маркова порядку $n-1$), коли на появу слова впливають n-1 попередніх слів, задаючи відповідні розподіли:
$$ P(x_k | x_{1:k-1}) = P(x_k | x_{k-n:k-1}),\ k > n $$
З статистичного боку n-грамна модель є досить хорошою, проте вона має дуже велику хибу, а саме те що кількість текстів для якісного навчання цієї моделі є дуже великим і експоненційно росте з ростом порядку моделі, але ріст порядку теж має свої недоліки при недостатньому об'ємі тексту для навчання. Тому n-грамні моделі згладжують за Лапласом. До прикладу оцінка максимальної правдоподібності для імовірності появи символа $y$ при попередніх $x_{1:n-1}$ в n-грамній моделі[20]:
$$ \hat{P}(X_k = y | X_{k-n:k-1} = x_{1:n-1}) = \frac{Cnt(x_{1:n-1},y)}{Cnt(x_{1:n-1})} $$
Згладжена за Лапласом оцінка:
$$ \hat{P}(X_k = y | X_{k-n:k-1} = x_{1:n-1}) = \frac{Cnt(x_{1:n-1},y) + c}{Cnt(x_{1:n-1}) + cm^{n-1}} $$
Де $Cnt(x_{1:n-1},y)$ - кількість n-грами $(x_{1:n-1},y)$ в тексті, $c$  - невелика константа згладжування, часто 0.5 або $1/m^{n-1}$, $m$ - розмір алфавіту.
Як було сказано друга оцінка краще у випадку коли розмір тексту для навчання відносно невеликий.

Розгялнемо також деякі сучасні підходи до побудови моделей мови. Зокрема була запропонована так звана позиційна модель мови[19]. Її суть полягає в тому, що окрім власне слів у тексті враховується їх абсолютна позиція. Статистична оцінка імовірності деякого слова $w$ на i-му місці буде мати вигляд:
$$ \hat{P}(X_i = w) = \frac{c'(w, i)}{\sum_{w' \in V} c'(w', i)} $$
Де $ c'(w, i) = \sum_{j=1}^{N} c(w,j) k(i,j) $. $V$ - словник, $c(w,j)$ - індикатор того, що слово $w$ стоїть на j-му місці(1 - якщо так, інакше 0), $k(i,j)$ - деяка вагова функція, не зростаюча відносно $|i-j|$ поширення слова на j позиції на сусідні з ним. Інтуїтивно можна розуміти, що слово вжите в деякому контексті буде збільшувати імовірність появи цього ж слова у цьому контексті.

Ще одним підходом до моделювання мови є застосування нейронних мереж для моделювання семантичної структури, зокрема такий підхід використовується в компанії \textit{Google} для створення систем перекладу та розпізнавання мови[21].

Далі буде перевірена узгодженість запропонованої моделі природної мови із стохастичними контекстно-вільними граматиками.
\section{Узгодженість запропонованої моделі з стохастичними контекстно-вільними граматиками}
В другому розділі було запропоновано нову модель стохастичної контекстно-вільної граматики та доведено еквівалентність моделі та стохастичних контекстно-вільних граматик.
Був розброблений програмний пакет на мові C++, шо дозволяє перевірити узгодженість моделі з стохастичними контекстно-вільними граматиками на практиці, його вихідний код наведений в \textit{додатку А}.
Перевіримо емпіричним шляхом узгодженість стохастичних граматик в 2-нормальній формі Грейбах та прихованих моделей Маркова другого порядку за спостереженнями зі стеком.
\begin{example}
  Нехай задано стохастичну граматику в 2-нормальній формі Грейбах:
\begin{align*}
  (0.2) A_1 \rightarrow a A_3 A_2 \\
  (0.8) A_1 \rightarrow c A_2 \\
  (0.5) A_2 \rightarrow a \\
  (0.5) A_2 \rightarrow b \\
  (0.3) A_3 \rightarrow d A_2 \\
  (0.3) A_3 \rightarrow c A_2 \\
  (0.4) A_3 \rightarrow a
\end{align*}
\end{example}

В результаті для ста речень згенерованих за допомогою стохастичної граматики в 2-нормальній формі Грейбах ми отримали наступний частотний розподіл:\\
\begin{verbatim}
  a a a : 5
  a a b : 1
  a c a a : 1
  a c a b : 3
  a c b a : 2
  a c b b : 3
  a d b a : 1
  a d b b : 1
  c a : 45
  c b : 38
\end{verbatim}

Для речень згенерованих за допомогою побудованої прихованої моделі Маркова другого порядку зі стеком маємо наступний розподіл:\\
\begin{verbatim}
  a a a : 1
  a a b : 2
  a c a a : 1
  a c b b : 2
  a d a a : 2
  a d a b : 2
  a d b a : 1
  a d b b : 3
  c a : 45
  c b : 41
\end{verbatim}

Як бачимо, статистики по двом згенерованим текстам є дуже близькими позаяк має місце доведена в другому розділі теорема про еквівалентність стохастичних контекстно-вільних граматик та прихованих моделей Маркова другого порядку за спостереженнями зі стеком.

\section{Перспективи нової моделі}
Запропонована модель природної мови, що заснована на прихованій моделі Маркова другого порядку за спостереженнями зі стеком є перспективною з точки зору подальших досліджень в криптоаналізі та обробці природних мов. Її можна застосовувати як модель відкритого тексту, оскільки її інформативність більша за вже запропоновані розглянуті статистичні моделі мови, так як будь-яка природна мова з певною долею точності може бути прийнята за контекстно-вільну. Модель неявно враховує зв'язки між всіма словами у реченні та семантичну структуру, таким чином статистично суттєво звужуючи можливу множину всіх можливих речень до множини більш імовірних речень. Цим обумовлюється інтерес з точки зору криптоаналізу. Варто зазначити, що модель в плані застосування є надзвичайно ефективною, оскільки складність граматичного виводу лінійно залежить від довжини речення, що виводиться.

Запропонована модель може мати не тільки криптографічне застосування, позаяк в біоінформатиці теж існує потреба в розробці ефективніших алгоритмів навчання ланцюжків ДНК та РНК. Таким чином однією з важливих  проблем для даної моделі є розробка нових алгоритмів її навчання за множиною текстів. Для цього можна використати наявний шаблон алгоритму Баума-Велша, модифікувавши його з врахуванням стеку.

\chapconclude{\ref{chap:practice}}
В розділі було розглянуто сучасні підходи до моделювання природної мови та емпіричним шляхом була перевірена узгодженість запропонованої моделі та стохастичних контекстно-вільних граматик. Також було розглянуто перспективи нової моделі та запропоновано ідеї для подальших досліджень.
